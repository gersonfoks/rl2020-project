{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# from evaluation.mc import *\n",
    "# from utils.misc import *\n",
    "# from policies import *\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EpsilonGreedyPolicy(object):\n",
    "    def __init__(self, actions, Q, epsilon):\n",
    "        self.actions = actions\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def get_probs(self, states, actions):\n",
    "        probs = np.full(len(states), self.epsilon/len(actions))    \n",
    "        index = np.random.choice(np.flatnonzero(self.Q[states[0]] == self.Q[states[0]].max()))\n",
    "        probs[index] += 1-self.epsilon\n",
    "        return probs\n",
    "\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        probs = self.get_probs([state for i in range(len(self.actions))], self.actions)\n",
    "        action = np.random.choice(self.actions, p=probs)\n",
    "        return action\n",
    "\n",
    "class RandomPolicy(object):\n",
    "\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "\n",
    "    def get_probs(self, states, actions):\n",
    "        probs = np.full(len(states), 1. / len(self.actions))\n",
    "        return probs\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        probs = self.get_probs([state for i in range(len(self.actions))], self.actions)\n",
    "\n",
    "        action = np.random.choice(self.actions, p=probs)\n",
    "        return action\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mc_weighted_importance_sampling(env, behavior_policy, target_policy, num_episodes, sampling_function,\n",
    "                                    discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given target policy using behavior policy and weighted importance sampling.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of current V and count of returns for each state\n",
    "    # to calculate an update.\n",
    "    V = defaultdict(float)\n",
    "\n",
    "    C = defaultdict(float)\n",
    "\n",
    "    returns_count = defaultdict(float)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    for i in tqdm(range(num_episodes)):\n",
    "        G = 0\n",
    "\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "        W = 1\n",
    "\n",
    "        T = len(states)\n",
    "        for t in range(T - 1, -1, -1):\n",
    "            state = states[t]\n",
    "            action = actions[t]\n",
    "            reward = rewards[t]\n",
    "\n",
    "            returns_count[state] += 1\n",
    "            G = discount_factor * G + reward\n",
    "            C[(state, action)] = C[(state, action)] + W\n",
    "            W = W * target_policy.get_probs([state], [action])[0] / behavior_policy.get_probs([state], [action])[0]\n",
    "\n",
    "            # Update formula\n",
    "            V[state] = V[state] + W * (G - V[state]) / C[(state, action)]\n",
    "\n",
    "            if W == 0:\n",
    "                break\n",
    "\n",
    "    return V\n",
    "def sample_episode(env, policy):\n",
    "    \"\"\"\n",
    "    A sampling routine. Given environment and a policy samples one episode and returns states, actions, rewards\n",
    "    and dones from environment's step function and policy's sample_action function as lists.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of lists (states, actions, rewards, dones). All lists should have same length.\n",
    "        Hint: Do not include the state after the termination in the list of states.\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        states.append(state)\n",
    "\n",
    "        action = policy.sample_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards, dones\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "1\n",
      "Discrete(2)\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "n = 7\n",
    "\n",
    "Q = np.zeros((n, 2))\n",
    "\n",
    "for state in range(n):\n",
    "    Q[state,  0] = 1\n",
    "\n",
    "from gym.envs.registration import register\n",
    "register( id='NChainN-v0', entry_point='gym.envs.toy_text:NChainEnv', kwargs={'n': n} )\n",
    "env = gym.make('NChainN-v0')\n",
    "env.n = n\n",
    "print(env.action_space)\n",
    "print(Q)\n",
    "\n",
    "# Let's sample some episodes\n",
    "actions = [0,1]\n",
    "target_policy = EpsilonGreedyPolicy(actions, Q, 0.1)\n",
    "behavior_policy = RandomPolicy(actions)\n",
    "print(1)\n",
    "for episode in range(3):\n",
    "    trajectory_data = sample_episode(env, behavior_policy)\n",
    "    print(\"Episode {}:\\nStates {}\\nActions {}\\nRewards {}\\nDones {}\\n\".format(episode,*trajectory_data))\n",
    "print(2)\n",
    "np.random.seed(42)\n",
    "V_10k = mc_weighted_importance_sampling(env,behavior_policy , target_policy, 10000, sample_episode)\n",
    "# V_500k = mc_weighted_importance_sampling(env, behavior_policy, target_policy, 500000, sample_episode)\n",
    "\n",
    "\n",
    "# Vs = [V_10k, V_500k]\n",
    "# x = np.arange(n)\n",
    "# fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "# for i in Vs:\n",
    "#     number_episodes = 1e4 if i == V_10k else 1e5\n",
    "#     plt.scatter(x,[i[j] for j in x], label= str(number_episodes))\n",
    "# plt.xlabel('States')\n",
    "# plt.ylabel('V')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "### First we run the env with random agent\n",
    "\n",
    "print(V_10k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

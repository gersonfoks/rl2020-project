{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from evaluation.mc import *\n",
    "from utils.misc import *\n",
    "from policies import *\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from evaluation.td import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_episode_lim(env, policy,limit=100):\n",
    "    \"\"\"\n",
    "    A sampling routine. Given environment and a policy samples one episode and returns states, actions, rewards\n",
    "    and dones from environment's step function and policy's sample_action function as lists.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of lists (states, actions, rewards, dones). All lists should have same length.\n",
    "        Hint: Do not include the state after the termination in the list of states.\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    for i in range(limit):\n",
    "        states.append(state)\n",
    "\n",
    "        action = policy.sample_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "class NChainEnv(gym.Env):\n",
    "    \"\"\"n-Chain environment\n",
    "    This game presents moves along a linear chain of states, with two actions:\n",
    "     0) forward, which moves along the chain but returns no reward\n",
    "     1) backward, which returns to the beginning and has a small reward\n",
    "    The end of the chain, however, presents a large reward, and by moving\n",
    "    'forward' at the end of the chain this large reward can be repeated.\n",
    "    At each action, there is a small probability that the agent 'slips' and the\n",
    "    opposite transition is instead taken.\n",
    "    The observed state is the current state in the chain (0 to n-1).\n",
    "    This environment is described in section 6.1 of:\n",
    "    A Bayesian Framework for Reinforcement Learning by Malcolm Strens (2000)\n",
    "    http://ceit.aut.ac.ir/~shiry/lecture/machine-learning/papers/BRL-2000.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, n=5, slip=0.2, small=2, large=10):\n",
    "        self.n = n\n",
    "        self.slip = slip  # probability of 'slipping' an action\n",
    "        self.small = small  # payout for 'backwards' action\n",
    "        self.large = large  # payout at end of chain for 'forwards' action\n",
    "        self.state = 0  # Start at beginning of the chain\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Discrete(self.n)\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        \n",
    "        if self.np_random.rand() < self.slip:\n",
    "            action = not action  # agent slipped, reverse action taken\n",
    "            \n",
    "            \n",
    "        #added \n",
    "        if self.state == 0 and action:\n",
    "            done = True\n",
    "        elif self.state == self.n - 1 and not action:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        #this part\n",
    "            \n",
    "        if action:  # 'backwards': go back to the beginning, get small reward\n",
    "            reward = self.small\n",
    "            self.state = 0\n",
    "        elif self.state < self.n - 1:  # 'forwards': go up along the chain\n",
    "            reward = 0\n",
    "            self.state += 1\n",
    "        else:  # 'forwards': stay at the end of the chain, collect large reward\n",
    "            reward = self.large\n",
    "        \n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500001/500001 [02:17<00:00, 3640.29it/s]\n",
      "100%|██████████| 500001/500001 [02:15<00:00, 3700.04it/s]\n",
      "100%|██████████| 500001/500001 [02:18<00:00, 3621.54it/s]\n",
      "100%|██████████| 500001/500001 [02:16<00:00, 3673.47it/s]\n",
      "100%|██████████| 500001/500001 [02:19<00:00, 3588.08it/s]\n",
      "100%|██████████| 500001/500001 [02:14<00:00, 3722.82it/s]\n",
      "100%|██████████| 500001/500001 [02:15<00:00, 3697.87it/s]\n",
      "100%|██████████| 500001/500001 [02:17<00:00, 3631.98it/s]\n",
      " 25%|██▌       | 125430/500001 [00:33<01:40, 3714.22it/s]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from environments.Nchain import NChainEnv\n",
    "from evaluation.mc import *\n",
    "from utils.misc import *\n",
    "from policies import *\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "for j in [5,10,15,20]:\n",
    "    for i in range(10):\n",
    "        n = j\n",
    "\n",
    "        Q = np.zeros((n, 2))\n",
    "\n",
    "        for state in range(n):\n",
    "            Q[state,  0] = 1\n",
    "\n",
    "        env = NChainEnv(n=n, slip = 0.0)\n",
    "        actions = [0, 1]\n",
    "        policy = EpsilonGreedyPolicy(actions, Q, 0.001)\n",
    "\n",
    "\n",
    "        random_policy = RandomPolicy(actions)\n",
    "\n",
    "        np.random.seed(i)\n",
    "        _, hist = mc_prediction(env, policy, 500001, sample_episode, save_every=1000, name=\"mc_nchain_{}_{}\".format(j,i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500001/500001 [01:28<00:00, 5623.70it/s]\n",
      "100%|██████████| 500001/500001 [01:29<00:00, 5614.74it/s]\n",
      "100%|██████████| 500001/500001 [01:29<00:00, 5610.28it/s]\n",
      "100%|██████████| 500001/500001 [01:28<00:00, 5640.20it/s]\n",
      "100%|██████████| 500001/500001 [01:29<00:00, 5599.55it/s]\n",
      "100%|██████████| 500001/500001 [01:28<00:00, 5636.29it/s]\n",
      "100%|██████████| 500001/500001 [01:29<00:00, 5599.56it/s]\n",
      "100%|██████████| 500001/500001 [01:28<00:00, 5630.55it/s]\n",
      "100%|██████████| 500001/500001 [01:28<00:00, 5624.15it/s]\n",
      "100%|██████████| 500001/500001 [01:29<00:00, 5615.93it/s]\n"
     ]
    }
   ],
   "source": [
    "for j in [5,15,20]:\n",
    "    for i in range(10):\n",
    "        n = j\n",
    "\n",
    "        Q = np.zeros((n, 2))\n",
    "\n",
    "        for state in range(n):\n",
    "            Q[state,  0] = 1\n",
    "\n",
    "        env = NChainEnv(n=n, slip = 0.0)\n",
    "        actions = [0, 1]\n",
    "        policy = EpsilonGreedyPolicy(actions, Q, 0.001)\n",
    "\n",
    "\n",
    "        random_policy = RandomPolicy(actions)\n",
    "\n",
    "        np.random.seed(i)\n",
    "        last, hist = mc_ordinary_importance_sampling(env, random_policy, policy, 500001, sample_episode, save_every=1000, name=\"mc_ordinary_nchain_{}_{}\".format(j,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500001/500001 [02:59<00:00, 2787.99it/s]\n",
      "100%|██████████| 500001/500001 [02:55<00:00, 2855.56it/s]\n",
      "100%|██████████| 500001/500001 [02:55<00:00, 2848.26it/s]\n",
      "100%|██████████| 500001/500001 [02:54<00:00, 2865.86it/s]\n",
      "100%|██████████| 500001/500001 [02:55<00:00, 2845.48it/s]\n",
      "100%|██████████| 500001/500001 [02:54<00:00, 2862.43it/s]\n",
      "100%|██████████| 500001/500001 [02:55<00:00, 2847.17it/s]\n",
      "100%|██████████| 500001/500001 [02:55<00:00, 2853.32it/s]\n",
      "100%|██████████| 500001/500001 [02:55<00:00, 2856.23it/s]\n",
      "100%|██████████| 500001/500001 [02:55<00:00, 2855.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from evaluation.td import *\n",
    "for j in [5,15,20]:\n",
    "    for i in range(10):\n",
    "        n = j\n",
    "\n",
    "        Q = np.zeros((n, 2))\n",
    "\n",
    "        for state in range(n):\n",
    "            Q[state,  0] = 1\n",
    "\n",
    "        env = NChainEnv(n=n, slip = 0.0)\n",
    "        actions = [0, 1]\n",
    "        policy = EpsilonGreedyPolicy(actions, Q, 0.001)\n",
    "\n",
    "\n",
    "        random_policy = RandomPolicy(actions)\n",
    "\n",
    "        np.random.seed(i)\n",
    "        _, V_hist = n_step_td_off_policy(env, random_policy, policy, 500001, sample_step, n=5, save_every=1000,name=\"td_nchain_{}_{}\".format(j,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500001/500001 [01:37<00:00, 5103.24it/s]\n",
      "100%|██████████| 500001/500001 [01:40<00:00, 4970.40it/s]\n",
      "100%|██████████| 500001/500001 [01:38<00:00, 5061.29it/s]\n",
      "100%|██████████| 500001/500001 [01:40<00:00, 4990.72it/s]\n",
      "100%|██████████| 500001/500001 [01:43<00:00, 4847.98it/s]\n",
      "100%|██████████| 500001/500001 [01:36<00:00, 5191.44it/s]\n",
      "100%|██████████| 500001/500001 [01:33<00:00, 5350.47it/s]\n",
      "100%|██████████| 500001/500001 [01:33<00:00, 5368.02it/s]\n",
      "100%|██████████| 500001/500001 [01:33<00:00, 5353.61it/s]\n",
      "100%|██████████| 500001/500001 [01:33<00:00, 5356.39it/s]\n"
     ]
    }
   ],
   "source": [
    "for j in [5,15,20]:\n",
    "    for i in range(10):\n",
    "        n = j\n",
    "\n",
    "        Q = np.zeros((n, 2))\n",
    "\n",
    "        for state in range(n):\n",
    "            Q[state,  0] = 1\n",
    "\n",
    "        env = NChainEnv(n=n, slip = 0.0)\n",
    "        actions = [0, 1]\n",
    "        policy = EpsilonGreedyPolicy(actions, Q, 0.001)\n",
    "\n",
    "\n",
    "        random_policy = RandomPolicy(actions)\n",
    "\n",
    "        np.random.seed(i)\n",
    "        last, hist = mc_weighted_importance_sampling(env, random_policy, policy, 500001, sample_episode, save_every=1000, name=\"mc_weighted_nchain_{}_{}\".format(j,i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlackJack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500001/500001 [01:18<00:00, 6356.09it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6342.03it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6372.53it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6350.70it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6347.75it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6333.92it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6333.32it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6356.48it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6354.90it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6339.37it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):    \n",
    "    player_values = [i for i in range(12, 22)]\n",
    "    dealer_values = [i for i in range(1,11)]\n",
    "\n",
    "\n",
    "    env = gym.make('Blackjack-v0')\n",
    "\n",
    "\n",
    "    # Let's sample some episodes\n",
    "\n",
    "    policy = SimpleBlackjackPolicy()\n",
    "    actions = [0,1]\n",
    "    blackjack_policy = RandomPolicy(actions)\n",
    "\n",
    "    np.random.seed(i)\n",
    "\n",
    "    last, hist = mc_ordinary_importance_sampling(env, blackjack_policy, SimpleBlackjackPolicy(), 500001, sample_episode, save_every=1000, name= \"mc_ordinary_bj_\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500001/500001 [01:18<00:00, 6408.24it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6390.67it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6386.17it/s]\n",
      "100%|██████████| 500001/500001 [01:17<00:00, 6436.85it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6368.14it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6392.21it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6392.73it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6388.43it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6390.52it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6368.10it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):    \n",
    "    player_values = [i for i in range(12, 22)]\n",
    "    dealer_values = [i for i in range(1,11)]\n",
    "\n",
    "\n",
    "    env = gym.make('Blackjack-v0')\n",
    "\n",
    "\n",
    "    # Let's sample some episodes\n",
    "\n",
    "    policy = SimpleBlackjackPolicy()\n",
    "    actions = [0,1]\n",
    "    blackjack_policy = RandomPolicy(actions)\n",
    "\n",
    "    np.random.seed(i)\n",
    "    _, hist = mc_prediction(env, SimpleBlackjackPolicy(), 500001, sample_episode, save_every=1000, name=\"mc_bj_\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500001/500001 [01:35<00:00, 5243.97it/s]\n",
      "100%|██████████| 500001/500001 [01:35<00:00, 5239.34it/s]\n",
      "100%|██████████| 500001/500001 [01:35<00:00, 5258.21it/s]\n",
      "100%|██████████| 500001/500001 [01:35<00:00, 5240.71it/s]\n",
      "100%|██████████| 500001/500001 [01:35<00:00, 5214.70it/s]\n",
      "100%|██████████| 500001/500001 [01:35<00:00, 5248.96it/s]\n",
      "100%|██████████| 500001/500001 [01:35<00:00, 5236.03it/s]\n",
      "100%|██████████| 500001/500001 [01:35<00:00, 5237.13it/s]\n",
      "100%|██████████| 500001/500001 [01:35<00:00, 5238.65it/s]\n",
      "100%|██████████| 500001/500001 [01:35<00:00, 5246.30it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):    \n",
    "    player_values = [i for i in range(12, 22)]\n",
    "    dealer_values = [i for i in range(1,11)]\n",
    "\n",
    "\n",
    "    env = gym.make('Blackjack-v0')\n",
    "\n",
    "\n",
    "    # Let's sample some episodes\n",
    "\n",
    "    policy = SimpleBlackjackPolicy()\n",
    "    actions = [0,1]\n",
    "    blackjack_policy = RandomPolicy(actions)\n",
    "\n",
    "    np.random.seed(i)\n",
    "    _, V_hist = n_step_td_off_policy(env, blackjack_policy, SimpleBlackjackPolicy(), 500001, sample_step, n=5, save_every=1000, name=\"td_bj_\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500001/500001 [01:18<00:00, 6403.95it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6401.21it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6401.84it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6392.57it/s]\n",
      "100%|██████████| 500001/500001 [01:17<00:00, 6440.97it/s]\n",
      "100%|██████████| 500001/500001 [01:17<00:00, 6417.36it/s]\n",
      "100%|██████████| 500001/500001 [01:17<00:00, 6421.85it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6409.23it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6378.54it/s]\n",
      "100%|██████████| 500001/500001 [01:18<00:00, 6409.93it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):    \n",
    "    player_values = [i for i in range(12, 22)]\n",
    "    dealer_values = [i for i in range(1,11)]\n",
    "\n",
    "\n",
    "    env = gym.make('Blackjack-v0')\n",
    "\n",
    "\n",
    "    # Let's sample some episodes\n",
    "\n",
    "    policy = SimpleBlackjackPolicy()\n",
    "    actions = [0,1]\n",
    "    blackjack_policy = RandomPolicy(actions)\n",
    "\n",
    "    np.random.seed(i)\n",
    "    _, hist = mc_weighted_importance_sampling(env, blackjack_policy, SimpleBlackjackPolicy(), 500001, sample_episode, save_every=1000, name= \"mc_weighted_bj_\"+str(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.78, # optimum = .8196\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500001/500001 [03:43<00:00, 2240.14it/s]\n",
      "100%|██████████| 500001/500001 [03:41<00:00, 2253.99it/s]\n",
      "100%|██████████| 500001/500001 [03:41<00:00, 2257.46it/s]\n",
      "100%|██████████| 500001/500001 [03:41<00:00, 2256.15it/s]\n",
      "100%|██████████| 500001/500001 [03:40<00:00, 2272.27it/s]\n",
      "100%|██████████| 500001/500001 [03:40<00:00, 2267.76it/s]\n",
      "100%|██████████| 500001/500001 [03:40<00:00, 2268.23it/s]\n",
      "100%|██████████| 500001/500001 [03:40<00:00, 2268.42it/s]\n",
      "100%|██████████| 500001/500001 [03:40<00:00, 2272.18it/s]\n",
      "100%|██████████| 500001/500001 [03:40<00:00, 2268.90it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "    env.is_slippery = False\n",
    "    action_map = {\n",
    "      0: \"D\",\n",
    "      1: \"R\",\n",
    "      2: \"D\",\n",
    "      3: \"L\",\n",
    "      4: \"D\",\n",
    "      5: \"U\",\n",
    "      6: \"D\",\n",
    "      7: \"U\",\n",
    "      8: \"R\",\n",
    "      9: \"D\",\n",
    "      10: \"D\",\n",
    "      11: \"U\",\n",
    "      12: \"U\",\n",
    "      13: \"R\",\n",
    "      14: \"R\",\n",
    "      15: \"D\",\n",
    "\n",
    "    }\n",
    "    index = {\"L\": 0, \"D\": 1, \"R\": 2, \"U\": 3}\n",
    "\n",
    "    Q = np.zeros((16, 4))\n",
    "    for state, action in action_map.items():\n",
    "        Q[state,  index[action]] =1\n",
    "\n",
    "    actions = [0,1,2,3]\n",
    "    target_policy =  EpsilonGreedyPolicy(actions, Q, 0.001)\n",
    "    behavior_policy = RandomPolicy(actions)\n",
    "\n",
    "    ### First we run the env with random agent\n",
    "\n",
    "    np.random.seed(i)\n",
    "    _, hist = mc_weighted_importance_sampling(env, behavior_policy, target_policy, 500001, sample_episode, save_every=1000, name= \"mc_weighted_fl_\"+str(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500001/500001 [03:37<00:00, 2295.76it/s]\n",
      "100%|██████████| 500001/500001 [03:38<00:00, 2289.14it/s]\n",
      "100%|██████████| 500001/500001 [03:37<00:00, 2299.82it/s]\n",
      "100%|██████████| 500001/500001 [03:37<00:00, 2297.16it/s]\n",
      "100%|██████████| 500001/500001 [03:42<00:00, 2250.94it/s]\n",
      "100%|██████████| 500001/500001 [03:42<00:00, 2242.18it/s]\n",
      "100%|██████████| 500001/500001 [03:46<00:00, 2206.44it/s]\n",
      "100%|██████████| 500001/500001 [03:43<00:00, 2232.16it/s]\n",
      "100%|██████████| 500001/500001 [03:36<00:00, 2307.42it/s]\n",
      "100%|██████████| 500001/500001 [03:36<00:00, 2306.24it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "    env.is_slippery = False\n",
    "    action_map = {\n",
    "      0: \"D\",\n",
    "      1: \"R\",\n",
    "      2: \"D\",\n",
    "      3: \"L\",\n",
    "      4: \"D\",\n",
    "      5: \"U\",\n",
    "      6: \"D\",\n",
    "      7: \"U\",\n",
    "      8: \"R\",\n",
    "      9: \"D\",\n",
    "      10: \"D\",\n",
    "      11: \"U\",\n",
    "      12: \"U\",\n",
    "      13: \"R\",\n",
    "      14: \"R\",\n",
    "      15: \"D\",\n",
    "\n",
    "    }\n",
    "    index = {\"L\": 0, \"D\": 1, \"R\": 2, \"U\": 3}\n",
    "\n",
    "    Q = np.zeros((16, 4))\n",
    "    for state, action in action_map.items():\n",
    "        Q[state,  index[action]] =1\n",
    "\n",
    "    actions = [0,1,2,3]\n",
    "    target_policy = EpsilonGreedyPolicy(actions, Q, 0.001)\n",
    "    behavior_policy = RandomPolicy(actions)\n",
    "\n",
    "    ### First we run the env with random agent\n",
    "\n",
    "    np.random.seed(i)\n",
    "    _, hist = mc_ordinary_importance_sampling(env, behavior_policy, target_policy, 500001, sample_episode, save_every=1000, name= \"mc_ordinary_fl_\"+str(i))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500001/500001 [04:17<00:00, 1938.88it/s]\n",
      "100%|██████████| 500001/500001 [04:17<00:00, 1939.26it/s]\n",
      "100%|██████████| 500001/500001 [04:19<00:00, 1927.13it/s]\n",
      "100%|██████████| 500001/500001 [04:28<00:00, 1860.75it/s]\n",
      "100%|██████████| 500001/500001 [04:19<00:00, 1927.89it/s]\n",
      "100%|██████████| 500001/500001 [04:18<00:00, 1932.34it/s]\n",
      "100%|██████████| 500001/500001 [04:19<00:00, 1929.63it/s]\n",
      "100%|██████████| 500001/500001 [04:19<00:00, 1929.31it/s]\n",
      "100%|██████████| 500001/500001 [04:22<00:00, 1907.54it/s]\n",
      "100%|██████████| 500001/500001 [04:30<00:00, 1847.27it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "    env.is_slippery = False\n",
    "    action_map = {\n",
    "      0: \"D\",\n",
    "      1: \"R\",\n",
    "      2: \"D\",\n",
    "      3: \"L\",\n",
    "      4: \"D\",\n",
    "      5: \"U\",\n",
    "      6: \"D\",\n",
    "      7: \"U\",\n",
    "      8: \"R\",\n",
    "      9: \"D\",\n",
    "      10: \"D\",\n",
    "      11: \"U\",\n",
    "      12: \"U\",\n",
    "      13: \"R\",\n",
    "      14: \"R\",\n",
    "      15: \"D\",\n",
    "\n",
    "    }\n",
    "    index = {\"L\": 0, \"D\": 1, \"R\": 2, \"U\": 3}\n",
    "\n",
    "    Q = np.zeros((16, 4))\n",
    "    for state, action in action_map.items():\n",
    "        Q[state,  index[action]] =1\n",
    "\n",
    "    actions = [0,1,2,3]\n",
    "    target_policy = EpsilonGreedyPolicy(actions, Q, 0.001)\n",
    "    behavior_policy = RandomPolicy(actions)\n",
    "\n",
    "    ### First we run the env with random agent\n",
    "\n",
    "    np.random.seed(i)\n",
    "    _, hist = n_step_td_off_policy(env, behavior_policy, target_policy, 500001, sample_step, save_every=1000, name= \"td_fl_\"+str(i))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500001/500001 [03:24<00:00, 2446.48it/s]\n",
      "100%|██████████| 500001/500001 [03:22<00:00, 2472.02it/s]\n",
      "100%|██████████| 500001/500001 [03:21<00:00, 2480.71it/s]\n",
      "100%|██████████| 500001/500001 [03:22<00:00, 2473.04it/s]\n",
      "100%|██████████| 500001/500001 [03:20<00:00, 2491.56it/s]\n",
      "100%|██████████| 500001/500001 [03:22<00:00, 2464.86it/s]\n",
      "100%|██████████| 500001/500001 [03:24<00:00, 2447.85it/s]\n",
      "100%|██████████| 500001/500001 [03:25<00:00, 2430.37it/s]\n",
      "100%|██████████| 500001/500001 [03:31<00:00, 2368.40it/s]\n",
      "100%|██████████| 500001/500001 [03:25<00:00, 2429.73it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "    env.is_slippery = False\n",
    "    action_map = {\n",
    "      0: \"D\",\n",
    "      1: \"R\",\n",
    "      2: \"D\",\n",
    "      3: \"L\",\n",
    "      4: \"D\",\n",
    "      5: \"U\",\n",
    "      6: \"D\",\n",
    "      7: \"U\",\n",
    "      8: \"R\",\n",
    "      9: \"D\",\n",
    "      10: \"D\",\n",
    "      11: \"U\",\n",
    "      12: \"U\",\n",
    "      13: \"R\",\n",
    "      14: \"R\",\n",
    "      15: \"D\",\n",
    "\n",
    "    }\n",
    "    index = {\"L\": 0, \"D\": 1, \"R\": 2, \"U\": 3}\n",
    "\n",
    "    Q = np.zeros((16, 4))\n",
    "    for state, action in action_map.items():\n",
    "        Q[state,  index[action]] =1\n",
    "\n",
    "    actions = [0,1,2,3]\n",
    "    target_policy = EpsilonGreedyPolicy(actions, Q, 0.001)\n",
    "    behavior_policy = RandomPolicy(actions)\n",
    "\n",
    "    ### First we run the env with random agent\n",
    "\n",
    "    np.random.seed(i)\n",
    "    _, hist = mc_prediction(env, target_policy, 500001, sample_episode, save_every=1000, name= \"mc_fl_\"+str(i))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
